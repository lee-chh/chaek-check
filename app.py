import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import MultiQueryRetriever
from dotenv import load_dotenv
import os

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ì±…ì²µ (Cloud Edition)", page_icon="â˜ï¸")
st.title("â˜ï¸ ì±…ì²µ (Cloud Edition)")
st.caption("Pinecone í´ë¼ìš°ë“œ DB ì—°ë™ ì™„ë£Œ! (Model: Large)")

# 2. Pinecone ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (Large ëª¨ë¸ ì ìš© í•„ìˆ˜!)
@st.cache_resource
def load_db():
    # âš ï¸ ì¤‘ìš”: ì•„ê¹Œ ingestí•  ë•Œ ì“´ ëª¨ë¸ê³¼ ë˜‘ê°™ì•„ì•¼ í•¨!
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    index_name = os.getenv("PINECONE_INDEX_NAME")
    
    # Pinecone ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ê²€ìƒ‰ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embeddings
    )
    return vectorstore

vectorstore = load_db()

# 3. ì„¸ì…˜ ë° ì²´ì¸ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state.messages = []
if "store" not in st.session_state:
    st.session_state.store = {}

def get_rag_chain():
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # ê¸°ë³¸ ê²€ìƒ‰ê¸° (ë²¡í„° ê²€ìƒ‰) - Large ëª¨ë¸ì´ë¼ ì„±ëŠ¥ êµ¿!
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # Multi-Query ê²€ìƒ‰ê¸° (BM25 ëŒ€ì‹  ì§ˆë¬¸ì„ 3ê°œë¡œ ë»¥íŠ€ê¸°í•´ì„œ ì»¤ë²„)
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm,
        include_original=True
    )

    # ëŒ€í™” ë§¥ë½ ì¸ì‹
    contextualize_q_system_prompt = """
    ì£¼ì–´ì§„ ì±„íŒ… ê¸°ë¡ê³¼ ìµœì‹  ì§ˆë¬¸ì„ ë³´ê³ ,
    ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ì´ ìˆë‹¤ë©´ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
    """
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = create_history_aware_retriever(
        llm, multi_query_retriever, contextualize_q_prompt
    )

    # ë‹µë³€ ìƒì„±
    qa_system_prompt = """
    ë‹¹ì‹ ì€ í•œêµ­ í”„ë¡œìŠ¤í¬ì¸ (Kë¦¬ê·¸, KBO) ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    ê·œì¹™:
    1. ë°˜ë“œì‹œ [Context]ì— ìˆëŠ” ë‚´ìš©ë§Œ ê°€ì§€ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
    2. ì§ˆë¬¸ì˜ ì˜ë„(ì¶•êµ¬ vs ì•¼êµ¬)ë¥¼ ì •í™•íˆ íŒŒì•…í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
    3. ê·œì •ì— ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
    
    [Context]:
    {context}
    """
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

def get_session_history(session_id: str):
    if session_id not in st.session_state.store:
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id]

# 4. UI êµ¬í˜„
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ê·œì •ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        rag_chain = get_rag_chain()
        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

        with st.spinner("â˜ï¸ Pinecone í´ë¼ìš°ë“œì—ì„œ ì°¾ëŠ” ì¤‘..."):
            result = conversational_rag_chain.invoke(
                {"input": prompt},
                config={"configurable": {"session_id": "current_session"}}
            )
            full_response = result["answer"]
            message_placeholder.markdown(full_response)
            
            if "context" in result and result["context"]:
                with st.expander("ğŸ“š ì°¸ê³ í•œ ê·œì • (Cloud Source)"):
                    seen = set()
                    for doc in result["context"]:
                        key = doc.metadata.get("source", "") + str(doc.metadata.get("page", ""))
                        if key not in seen:
                            seen.add(key)
                            fname = os.path.basename(doc.metadata.get("source", "Unknown"))
                            # Pineconeì€ page ìˆ«ìê°€ floatë¡œ ì €ì¥ë  ë•Œê°€ ìˆì–´ì„œ int ë³€í™˜
                            page = int(doc.metadata.get("page", 0)) + 1
                            st.caption(f"ğŸ“„ {fname} (p.{page})")
                            st.text(doc.page_content[:100] + "...")

    st.session_state.messages.append({"role": "assistant", "content": full_response})